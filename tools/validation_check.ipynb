{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import wandb\n",
    "\n",
    "from utils_.set_path import *\n",
    "from utils_.loss import FocalLoss, LabelSmoothingLoss, F1Loss\n",
    "from utils_.get_class_weight import calc_class_weight\n",
    "from runner.pytorch_timm import TimmModel\n",
    "import matplotlib.pyplot as plt\n",
    "# from runner.train_runner import CustomTrainer\n",
    "# from data.dataset import CustomTrainDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(909)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "def get_transforms(img_size, transforms):\n",
    "    if transforms:\n",
    "         return A.Compose([\n",
    "                    # A.CenterCrop(height=384, width=384, p=1.0),\n",
    "                    A.Resize(img_size[0], img_size[1]),\n",
    "                    A.HorizontalFlip(p=0.5),\n",
    "                    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n",
    "                    ToTensorV2()\n",
    "                    ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "                    # A.CenterCrop(height=384, width=384, p=1.0),\n",
    "                    A.Resize(img_size[0], img_size[1]),\n",
    "                    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n",
    "                    ToTensorV2()\n",
    "                    ])\n",
    "\n",
    "class CustomTrainDataset(Dataset):\n",
    "    _mask_labels = {\"mask1\": 0, \"mask2\": 0, \"mask3\": 0, \"mask4\": 0, \"mask5\": 0, \"incorrect_mask\": 1, \"normal\": 2}\n",
    "    _gender_labels = {\"male\": 0, \"female\": 1}\n",
    "\n",
    "    def __init__(self, model_name, data_dir, folder_list, resize, transforms):\n",
    "        self.model_name = model_name\n",
    "        self.data_dir = data_dir\n",
    "        self.folder_list = folder_list\n",
    "        self.resize = resize\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.image_paths = []\n",
    "        self.mask_labels, self.gender_labels, self.age_labels = [], [], []\n",
    "        self.setup()\n",
    "        \n",
    "\n",
    "    def setup(self):\n",
    "        for folder in self.folder_list:\n",
    "            img_folder = os.path.join(self.data_dir, folder) # ('/workspace/data/train/image', 000004_male_Asian_54)\n",
    "            for file_name in os.listdir(img_folder): # ('mask1.jpg', 'mask2.jpg', ... )\n",
    "                _file_name, _ = os.path.splitext(file_name) # ('mask1', '.jpg')\n",
    "                img_path = os.path.join(self.data_dir, folder, file_name)  # (resized_data, 000004_male_Asian_54, mask1.jpg)\n",
    "                \n",
    "                _, gender, _, age = folder.split(\"_\")\n",
    "                # mask_label = self._mask_labels[_file_name]                \n",
    "                gender_label = self._gender_labels[gender]\n",
    "                age = int(age)\n",
    "                age_label = 0 if age < 30 else 1 if age < 60 else 2\n",
    "                \n",
    "                self.image_paths.append(img_path)\n",
    "                if self.model_name == 'Mask': self.mask_labels.append(0)\n",
    "                elif self.model_name == 'Gender': self.gender_labels.append(gender_label)\n",
    "                elif self.model_name == 'Age': \n",
    "                    self.age_labels.append(age_label)\n",
    "    def __getitem__(self, index):\n",
    "        assert self.transforms is not None, \"transform 을 주입해주세요\"\n",
    "        \n",
    "        image = cv2.imread(self.image_paths[index])\n",
    "        trfm = get_transforms(self.resize, self.transforms)\n",
    "        image = trfm(image=image)['image']\n",
    "        \n",
    "        if self.model_name == 'Mask':\n",
    "            mask_label = self.mask_labels[index]\n",
    "            return image, mask_label\n",
    "        elif self.model_name == 'Gender':\n",
    "            gender_label = self.gender_labels[index]\n",
    "            return image, gender_label\n",
    "        elif self.model_name == 'Age':\n",
    "            age_label = self.age_labels[index]\n",
    "            return image, age_label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, img_paths, resize, transforms=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.resize = resize\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.img_paths[index])\n",
    "        trfm = get_transforms(self.resize, self.transforms)\n",
    "        image = trfm(image=image)['image']\n",
    "            \n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Age'\n",
    "image_resize = [512, 384]\n",
    "gc.collect() # python 자원 관리 \n",
    "torch.cuda.empty_cache() # gpu 자원관리\n",
    "\n",
    "data_dir = TRAIN_IMG_FOLDER_PATH\n",
    "model_dir = MODEL_SAVE_PATH\n",
    "train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "miss_label = ['006424', '006339', '003713', '003437', '003421', '003399', '003294', '003169', '003113', '003014', '001520', '001266', '000725', '000647']\n",
    "drop_index = []\n",
    "for ml in miss_label:\n",
    "    drop_index.append(train_df[train_df['id']==ml].index[0])\n",
    "for index in drop_index:\n",
    "    train_df = train_df.drop(index)\n",
    "train_, val_ = train_test_split(train_df, test_size=0.2, random_state=909)\n",
    "\n",
    "train_dataset = CustomTrainDataset(model_name, data_dir, train_['path'].values, image_resize, transforms=True)\n",
    "val_dataset = CustomTrainDataset(model_name, data_dir, val_['path'].values, image_resize, transforms=False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 32, shuffle=True, num_workers=8)\n",
    "valid_dataloader = DataLoader(val_dataset, batch_size = 32, shuffle=False, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python train.py --seed 909 --epochs 100 --resize 256 192 --batch_size 32 --criterion CrossEntropy --model resnet34 --lr 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import timm\n",
    "\n",
    "class TimmModel(nn.Module):\n",
    "    def __init__(self, model, num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.pretrained = pretrained\n",
    "        \n",
    "        self.model = timm.create_model(model, pretrained=self.pretrained)\n",
    "        self.fc = nn.Sequential(nn.Dropout(p=0.2, inplace=True),\n",
    "                               nn.Linear(1000, 512),\n",
    "                               nn.Dropout(p=0.2, inplace=True),\n",
    "                               nn.Linear(512, self.num_classes),\n",
    "                               )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "model = TimmModel('resnet34', num_classes=3, pretrained=True).to(device)\n",
    "model.load_state_dict(torch.load(\"/opt/ml/geunuk/model/Project54/Age_[resnet34]_[score0.8015]_[loss0.3479].pt\"))\n",
    "# model = nn.DataParallel(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer():\n",
    "    def __init__(self, args = None, model = None, valid_dataloader = None, optimizer = None, scheduler = None, criterion = None, device = None):\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.criterion = criterion\n",
    "        self.device = device \n",
    "\n",
    "    def validation(self):\n",
    "        self.model.eval()\n",
    "        val_loss = []\n",
    "        preds, trues = [], []\n",
    "        print('='*25, f'VALID', '='*25)\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(iter(self.valid_dataloader), bar_format='{l_bar}{bar:50}{r_bar}{bar:-10b}'):\n",
    "                imgs = imgs.float().to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                logit = self.model(imgs)\n",
    "\n",
    "                loss = self.criterion(logit, labels)\n",
    "\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "                preds += logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "                trues += labels.detach().cpu().numpy().tolist()\n",
    "\n",
    "            _val_loss = np.mean(val_loss)\n",
    "            _val_acc = f1_score(trues, preds, average='macro')\n",
    "        return _val_loss, _val_acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation f1 score 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_weight = calc_class_weight(train_dataset, model_name, 3, device)\n",
    "#criterion = FocalLoss(weight=class_weight).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-3)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, threshold_mode='abs',min_lr=1e-9, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(model = model, valid_dataloader = valid_dataloader, criterion=criterion, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab33deaf3e346909a6c5c2ce4fb30bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "targets_img = None\n",
    "preds, targets = torch.Tensor().to(device), torch.Tensor().to(device)\n",
    "preds_all, targets_all = torch.Tensor().to(torch.int32).to(device), torch.Tensor().to(torch.int32).to(device)\n",
    "with torch.no_grad():\n",
    "    for X, y in tqdm(iter(valid_dataloader)):\n",
    "        model.eval()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        pred = model(X)\n",
    "        pred = pred.argmax(dim=1)\n",
    "        indices = (pred != y).nonzero().squeeze()\n",
    "        preds_all = torch.cat((preds_all, pred))\n",
    "        targets_all = torch.cat((targets_all, y))\n",
    "        if indices.nelement() != 0:\n",
    "            target_img = X[indices]\n",
    "            if target_img.dim() == 3:\n",
    "                target_img = target_img.unsqueeze(dim=0)\n",
    "            \n",
    "            if targets_img is None:\n",
    "                targets_img = target_img\n",
    "            else:\n",
    "                targets_img = torch.cat((targets_img, target_img), dim=0)\n",
    "            \n",
    "            ps = pred[indices]\n",
    "            ys = y[indices]\n",
    "            if ys.dim() == 0:\n",
    "                ys = ys.unsqueeze(dim=0)\n",
    "                ps = ps.unsqueeze(dim=0)\n",
    "            preds = torch.cat((preds, ps))\n",
    "            targets = torch.cat((targets, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3766"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8134464235227594"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(targets_all.cpu(), preds_all.cpu(), average=\"macro\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class activation map 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization = {}\n",
    "def hook(module, input, output):\n",
    "    if module == model.model.layer4:\n",
    "        visualization[\"test\"] = output\n",
    "        \n",
    "for module in model.modules():\n",
    "    module.register_forward_hook(hook)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm_image(img, size):\n",
    "    c1 = torch.zeros(1, *size) + 0.229\n",
    "    c2 = torch.zeros(1, *size) + 0.224\n",
    "    c3 = torch.zeros(1, *size) + 0.225\n",
    "    std_cn = torch.cat((c1, c2, c3), dim=0)\n",
    "\n",
    "    c1 = torch.zeros(1, *size) + 0.485\n",
    "    c2 = torch.zeros(1, *size) + 0.456\n",
    "    c3 = torch.zeros(1, *size) + 0.406\n",
    "    std_mean = torch.cat((c1, c2, c3), dim=0)\n",
    "    \n",
    "    return img.cpu() * std_cn + std_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_iter = iter(valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action=\"ignore\")\n",
    "plt.figure(figsize=(15, 25))\n",
    "with torch.no_grad():\n",
    "    for i in range(32):\n",
    "        model.eval()\n",
    "        image_idx=i\n",
    "\n",
    "        label = sample[1][image_idx]\n",
    "        sample_image = sample[0][image_idx].unsqueeze(0)\n",
    "\n",
    "        output = model(sample_image.cuda())\n",
    "        output = output.argmax(dim=-1)\n",
    "        \n",
    "\n",
    "        activation_map = list(visualization.values())\n",
    "        activation_map = activation_map[0]\n",
    "\n",
    "        class_weight = model.fc[3].weight[int(output)].unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        cam_ = torch.squeeze(activation_map * class_weight)\n",
    "        cam = torch.sum(cam_, axis=0)\n",
    "        cam = cam.detach().cpu().numpy()\n",
    "\n",
    "        final_cam = cv2.resize(cam, (384, 412), interpolation=cv2.INTER_CUBIC)\n",
    "        plt.subplot(7, 5, i+1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f't:{label.item()}, p:{output.item()}')\n",
    "        plt.imshow(denorm_image(sample_image[0], sample_image[0].shape[1:]).permute(1, 2, 0).flip(2))\n",
    "        plt.imshow(final_cam, alpha=0.5, cmap=\"jet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set 잘못된 prediction 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "misprediction_count = {\n",
    "    (0, 1) : 0,\n",
    "    (0, 2) : 0,\n",
    "    (1, 0) : 0,\n",
    "    (1, 2) : 0,\n",
    "    (2, 0) : 0,\n",
    "    (2, 1) : 0\n",
    "}\n",
    "misprediction_tostring = {\n",
    "    (0, 1) : \"pred <30, label 30to60\",\n",
    "    (0, 2) : \"pred <30, label >60\",\n",
    "    (1, 0) : \"pred 30to60, label <30\",\n",
    "    (1, 2) : \"pred 30to60, label >60\",\n",
    "    (2, 0) : \"pred >60, label <30\",\n",
    "    (2, 1) : \"pred >60, label 30to60\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2336591dbd334e9abb6c317b443e2c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for pred, label in tqdm(zip(preds, targets)):\n",
    "    misprediction_count[(pred.item(), label.item())] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(list(zip(preds.to(torch.uint8).cpu(), targets.to(torch.uint8).cpu())))\n",
    "indices = np.where((test[:, 0] == 0) & (test[:, 1] == 1))\n",
    "indices = indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): 68, (0, 2): 0, (1, 0): 45, (1, 2): 120, (2, 0): 0, (2, 1): 108}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misprediction_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred <30, label 30to60: 68\n",
      "pred <30, label >60: 0\n",
      "pred 30to60, label <30: 45\n",
      "pred 30to60, label >60: 120\n",
      "pred >60, label <30: 0\n",
      "pred >60, label 30to60: 108\n"
     ]
    }
   ],
   "source": [
    "for key, value in misprediction_count.items():\n",
    "    print(f'{misprediction_tostring[key]}: {value}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set 잘못된 prediction 이미지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = torch.zeros(1, 256, 192) + 0.229\n",
    "c2 = torch.zeros(1, 256, 192) + 0.224\n",
    "c3 = torch.zeros(1, 256, 192) + 0.225\n",
    "std_cn = torch.cat((c1, c2, c3), dim=0)\n",
    "\n",
    "c1 = torch.zeros(1, 256, 192) + 0.485\n",
    "c2 = torch.zeros(1, 256, 192) + 0.456\n",
    "c3 = torch.zeros(1, 256, 192) + 0.406\n",
    "std_mean = torch.cat((c1, c2, c3), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_dict = {0: \"male\", 1:\"female\"}\n",
    "class_dict = {0: \"<30\", 1:\"30to60\", 2:\">60\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts_train = {i:0 for i in range(3)}\n",
    "class_counts_val = {i:0 for i in range(3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, target in val_dataset:\n",
    "    class_counts_val[target]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 7280, 1: 6839, 2: 1001}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1687, 1: 1750, 2: 343}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, target in train_dataset:\n",
    "    class_counts_train[target]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(list(zip(preds.to(torch.uint8).cpu(), targets.to(torch.uint8).cpu())))\n",
    "indices = np.where((test[:, 0] == 0) & (test[:, 1] == 1))\n",
    "indices = indices[0]\n",
    "\n",
    "plt.figure(figsize=(30, 90))\n",
    "for idx, (img, pred, targ) in enumerate(zip(targets_img[indices], preds[indices], targets[indices])):\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(20, 10, idx+1)\n",
    "    plt.title(f\"pred:{class_dict[pred.item()]}, label:{class_dict[targ.item()]}\", color=\"white\")\n",
    "    plt.imshow((img.cpu() * std_cn + std_mean).permute(1, 2, 0).flip(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(list(zip(preds.to(torch.uint8).cpu(), targets.to(torch.uint8).cpu())))\n",
    "indices = np.where((test[:, 0] == 0) & (test[:, 1] == 2))\n",
    "indices = indices[0]\n",
    "\n",
    "plt.figure(figsize=(30, 90))\n",
    "for idx, (img, pred, targ) in enumerate(zip(targets_img[indices], preds[indices], targets[indices])):\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(20, 10, idx+1)\n",
    "    plt.title(f\"pred:{class_dict[pred.item()]}, label:{class_dict[targ.item()]}\",color=\"white\")\n",
    "    plt.imshow((img.cpu() * std_cn + std_mean).permute(1, 2, 0).flip(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(list(zip(preds.to(torch.uint8).cpu(), targets.to(torch.uint8).cpu())))\n",
    "indices = np.where((test[:, 0] == 1) & (test[:, 1] == 0))\n",
    "indices = indices[0]\n",
    "\n",
    "plt.figure(figsize=(30, 90))\n",
    "for idx, (img, pred, targ) in enumerate(zip(targets_img[indices], preds[indices], targets[indices])):\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(20, 10, idx+1)\n",
    "    plt.title(f\"pred:{class_dict[pred.item()]}, label:{class_dict[targ.item()]}\", color=\"white\")\n",
    "    plt.imshow((img.cpu() * std_cn + std_mean).permute(1, 2, 0).flip(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(list(zip(preds.to(torch.uint8).cpu(), targets.to(torch.uint8).cpu())))\n",
    "indices = np.where((test[:, 0] == 1) & (test[:, 1] == 2))\n",
    "indices = indices[0]\n",
    "\n",
    "plt.figure(figsize=(30, 90))\n",
    "for idx, (img, pred, targ) in enumerate(zip(targets_img[indices], preds[indices], targets[indices])):\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(20, 10, idx+1)\n",
    "    plt.title(f\"pred:{class_dict[pred.item()]}, label:{class_dict[targ.item()]}\", color=\"white\")\n",
    "    plt.imshow((img.cpu() * std_cn + std_mean).permute(1, 2, 0).flip(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(list(zip(preds.to(torch.uint8).cpu(), targets.to(torch.uint8).cpu())))\n",
    "indices = np.where((test[:, 0] == 2) & (test[:, 1] == 0))\n",
    "indices = indices[0]\n",
    "\n",
    "plt.figure(figsize=(30, 90))\n",
    "for idx, (img, pred, targ) in enumerate(zip(targets_img[indices], preds[indices], targets[indices])):\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(20, 10, idx+1)\n",
    "    plt.title(f\"pred:{class_dict[pred.item()]}, label:{class_dict[targ.item()]}\", color=\"white\")\n",
    "    plt.imshow((img.cpu() * std_cn + std_mean).permute(1, 2, 0).flip(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(list(zip(preds.to(torch.uint8).cpu(), targets.to(torch.uint8).cpu())))\n",
    "indices = np.where((test[:, 0] == 2) & (test[:, 1] == 1))\n",
    "indices = indices[0]\n",
    "\n",
    "plt.figure(figsize=(30, 90))\n",
    "for idx, (img, pred, targ) in enumerate(zip(targets_img[indices], preds[indices], targets[indices])):\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(20, 10, idx+1)\n",
    "    plt.title(f\"pred:{class_dict[pred.item()]}, label:{class_dict[targ.item()]}\", color=\"white\")\n",
    "    plt.imshow((img.cpu() * std_cn + std_mean).permute(1, 2, 0).flip(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
